#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as nm
import pyspark
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType
from pyspark.sql.functions import col, column
from pyspark.sql.functions import expr
from pyspark.sql.functions import split
from pyspark.sql import Row
import csv
import pyspark.sql.functions as F
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from datetime import datetime


# In[2]:


from decision_tree_plot.decision_tree_parser import decision_tree_parse
from decision_tree_plot.decision_tree_plot import plot_trees


# In[3]:


ss = SparkSession.builder.appName("Project").getOrCreate()


# In[4]:


bc_schema = StructType([ StructField("ID", IntegerType(), False ),                         StructField("Case Number", StringType(), False),                         StructField("Date", StringType(), False ),                         StructField("Block", StringType(), False ),                         StructField("IUCR", StringType(), False),                         StructField("Primary Type", StringType(), False),                         StructField("Description", StringType(), False),                        StructField("Location Description", StringType(), False),                         StructField("Arrest", StringType(), False),                         StructField("Domestic", StringType(), False),                         StructField("District", StringType(), False) ,                        StructField("Ward", StringType(), False ),                         StructField("Community Area", StringType(), False ),                         StructField("FBI Code", StringType(), False),                         StructField("Year", StringType(), False),                         StructField("Latitude", StringType(), False),                        StructField("Longitude", StringType(), False)
                           ])


# In[5]:


# new_file3.csv is the file generated by using Suvarna's code
data = ss.read.csv("./new_file3.csv",schema=bc_schema,header=True, inferSchema=False)


# In[9]:


data = data.drop('Year',)
data = data.drop('Block')
data = data.drop('Case Number')
data = data.drop('ID')


# In[8]:


# Get the names of all the columns in the DataFrame
column_names = data.columns

# Print the column names
for column_name in column_names:
    print(column_name)


# In[10]:


data.show(5)


# In[11]:


columns_to_index = ["Date", "Primary Type", "Description", "IUCR",
                    "Location Description", "Arrest", "Domestic", "District","Ward","FBI Code","Community Area", 
                    "Latitude", "Longitude"]
label_indexers = {col: StringIndexer(inputCol=col, outputCol=f"indexed_{col.replace(' ', '_').lower()}").fit(data) for col in columns_to_index}


# In[12]:


# Apply transformations to the data
transformed_data = data
for indexer in label_indexers.values():
    transformed_data = indexer.transform(transformed_data)


# In[13]:


transformed_data.printSchema()


# GPT

# In[ ]:





# In[ ]:





# In[ ]:





# My code

# In[14]:


transformed_data2 = transformed_data.select("indexed_date","indexed_iucr",                                           "indexed_primary_type","indexed_description","indexed_location_description",                                           "indexed_arrest","indexed_domestic","indexed_district","indexed_ward",                                           "indexed_fbi_code","indexed_community_area","indexed_latitude","indexed_longitude")


# In[20]:


input_features = ["indexed_date","indexed_iucr",                                           "indexed_primary_type","indexed_description","indexed_location_description",                                           "indexed_domestic","indexed_district","indexed_ward",                                           "indexed_fbi_code","indexed_community_area","indexed_latitude","indexed_longitude"]


# In[21]:


transformed_data2.show(3)


# In[22]:


assembler = VectorAssembler(inputCols=input_features, outputCol="features_for_arrest")


# In[23]:


assembler


# In[24]:


vectorized_data = assembler.transform(transformed_data2)


# In[25]:


vectorized2_data = vectorized_data.select("features_for_arrest",'indexed_arrest')
vectorized2_data.show(5)


# Decision Tree Learning

# In[26]:


trainingData,testData= vectorized_data.randomSplit([0.75, 0.25], seed=1237)


# In[28]:


from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml import Pipeline
dt=DecisionTreeClassifier(featuresCol="features_for_arrest", labelCol="indexed_arrest", maxDepth=6, minInstancesPerNode=2, maxBins=400000)


# In[ ]:


dt_model = dt.fit(trainingData)


# In[ ]:


dt_model


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




