{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4087bd0-5dd0-47f9-90e9-2f9fe30a88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as nm\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "import csv\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString, StandardScaler, PCA\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145bd3d7-5113-450c-9b83-2d8c99467888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce7cd2-0e75-45f9-a876-b440dd12dde2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784e8a2-1b8f-4ac1-9235-1bba7b669a4d",
   "metadata": {},
   "source": [
    "## Initialize a SparkSession, define the csv Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9237ef82-410c-405d-8382-3b716e17a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.appName(\"Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e73d57f-5cf4-458d-b662-e02e70fac54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ StructField(\"ID\", IntegerType(), False ), \\\n",
    "                        StructField(\"Case Number\", StringType(), False), \\\n",
    "                        StructField(\"Date\", StringType(), False ), \\\n",
    "                        StructField(\"Block\", StringType(), False ), \\\n",
    "                        StructField(\"IUCR\", StringType(), False), \\\n",
    "                        StructField(\"Primary Type\", StringType(), False), \\\n",
    "                        StructField(\"Description\", StringType(), False),\\\n",
    "                        StructField(\"Location Description\", StringType(), False), \\\n",
    "                        StructField(\"Arrest\", StringType(), False), \\\n",
    "                        StructField(\"Domestic\", StringType(), False), \\\n",
    "                        StructField(\"District\", StringType(), False) ,\\\n",
    "                        StructField(\"Ward\", StringType(), False ), \\\n",
    "                        StructField(\"Community Area\", StringType(), False ), \\\n",
    "                        StructField(\"FBI Code\", StringType(), False), \\\n",
    "                        StructField(\"Year\", StringType(), False), \\\n",
    "                        StructField(\"Latitude\", StringType(), False),\\\n",
    "                        StructField(\"Longitude\", StringType(), False)\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3793f47a-7f40-4d0d-828d-1829c0fd44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_file3.csv is the file generated by using Suvarna's code\n",
    "data = ss.read.csv(\"./new_file3.csv\",schema=schema,header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d854774f-3dd5-4039-a5f7-575999474706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Year').drop('Block').drop('Case Number').drop('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858e103-8c54-464c-80db-a7b92cb29195",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07a9fe-c59d-432d-9087-3c763eeb3544",
   "metadata": {},
   "source": [
    "## Apply PCA Reduction\n",
    "#### Reason: I tried directly using Lab8's code before. However, I continously got an error saying the data's cardinality is too high and aborted the task, which means the dataset's size is too large. To reduce the dataset's size, I need to use PCA reduction first to reduce the size and then use the reduced data to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91439939-43e6-462d-8bab-524fab27ce0f",
   "metadata": {},
   "source": [
    "`Step 1: Transforms a column of string to a new column of index (type double)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b640875-f12d-42a5-b51d-a46d896a4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block will create a new column for each indexed column, for example, will create a 'i_date' for indexed 'Date' column\n",
    "columns_to_index = [\"Date\", \"Primary Type\", \"Description\", \"IUCR\",\n",
    "                    \"Location Description\", \"Arrest\", \"Domestic\", \"District\",\"Ward\",\"FBI Code\",\"Community Area\", \n",
    "                    \"Latitude\", \"Longitude\"]\n",
    "label_indexers = {col: StringIndexer(inputCol=col, outputCol=f\"i_{col.replace(' ', '_').lower()}\").fit(data) for col in columns_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9657ed0-123e-4699-902a-06a5afb9f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block will transform data\n",
    "transformed_data = data\n",
    "for indexer in label_indexers.values():\n",
    "    transformed_data = indexer.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ba12eb-b431-429f-850c-501235e55c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2 will contain the indexed values only, which means they are all integer -> ready for training\n",
    "data2 = transformed_data.select(\"i_date\",\"i_iucr\",\"i_primary_type\",\"i_description\",\"i_location_description\",\\\n",
    "                    \"i_arrest\",\"i_domestic\",\"i_district\",\"i_ward\",\"i_fbi_code\",\"i_community_area\",\"i_latitude\",\"i_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5dac44-1276-49c7-a7b4-28886d3a4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_features = [\"i_date\",\"i_iucr\",\"i_primary_type\",\"i_description\",\"i_location_description\",\\\n",
    "                    \"i_domestic\",\"i_district\",\"i_ward\",\"i_fbi_code\",\"i_community_area\",\"i_latitude\",\"i_longitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20036de-f310-4d0f-a824-312284637a44",
   "metadata": {},
   "source": [
    "`Step 2: Create a vector-assembler to combine individual features into a vector in a new column, which called 'features_for_arrest'.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ea258b-0996-45cb-b64c-cb37f573b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features_for_arrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7a6941-9ac0-4bf4-8b9b-326d2378641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_data = assembler.transform(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dffa69-ad6b-4a9b-b3f5-c0d02ab00e65",
   "metadata": {},
   "source": [
    "`Step 3: Use a StandardScaler to center the data - saved the centered data into a new column called \"scaled_features\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c2483d-3fcc-48ec-81a1-94959aa48d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features_for_arrest\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_data)\n",
    "scaled_data = scaler_model.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9e53429-8471-4658-80c5-6ef7ad6acf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance:  0.9360433841638934\n"
     ]
    }
   ],
   "source": [
    "# When k=8, its variance is closest to 0.9 && > 0.9\n",
    "pca = PCA(k=8,inputCol=\"scaled_features\",outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(scaled_data)\n",
    "result = model.transform(scaled_data)\n",
    "explained_variance = model.explainedVariance\n",
    "print(\"Explained Variance: \", sum(explained_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98200bfc-4101-4f40-b37e-74cff7b62822",
   "metadata": {},
   "source": [
    "`By that, We got the PCA reduction data: result `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c256e-344a-40f0-b4f9-ba25d3c8b0b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd85668-3ebf-4090-a5a5-1902b7d8c594",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36529c5-0b12-46b7-ac78-a936c406f02e",
   "metadata": {},
   "source": [
    "####  Train the model with the reduced data. I used sklearn instead of PySpark DecisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1e1990-34b6-4958-9c17-005324407d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47467d08-5c84-49f4-a769-7333759f45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "# \"pcaFeatures\" is the hyperparameter that predicts arrest or not\n",
    "# \"i_arrest\" is our target variable\n",
    "pandas_df = result.select(\"pcaFeatures\", \"i_arrest\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "442ed721-8354-4c78-9f75-b90084929112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PCA features and target variable\n",
    "X = pandas_df[\"pcaFeatures\"].tolist()\n",
    "y = pandas_df[\"i_arrest\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc4b1ac3-15fc-49ca-a163-2a35d44d753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "852fcdc2-5d91-4e27-978c-2348139ec87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data again just in case\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c8c9a14-6fee-4c25-b6cc-0404955bca46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011d125-4290-4819-964f-1efa4eea3eb4",
   "metadata": {},
   "source": [
    "`Above is the model that predicts the value of Arrest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9a262f4-0442-42fe-b543-ba63fb24f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db3b00-74ad-486c-8524-fa1965c82c5b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba2333-c0df-4c68-a2dc-fe44ccf923c6",
   "metadata": {},
   "source": [
    "## Evaluate the model (Don't run the code until testing phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9862539c-4c6c-4e36-b14f-65ce824ca723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test_data to evaluate our model \n",
    "'''\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
